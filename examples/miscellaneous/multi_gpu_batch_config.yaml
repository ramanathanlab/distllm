# Multi-GPU Configuration for 8 GPUs with Batching Support
# Strategy: Single vLLM server with tensor parallelism across 8 GPUs + request batching

questions_file: "test_questions_simple.json"

model:
  generator:
    generator_type: "vllm"
  
  generator_settings:
    # Traditional fields
    server: "localhost"
    port: 8000
    api_key: "CELS"
    model: "meta-llama/Llama-2-7b-chat-hf"
    temperature: 0.0
    max_tokens: 1024
    
    # Local vLLM server configuration
    boot_local: true
    hf_model_id: "meta-llama/Llama-2-7b-chat-hf"  # Your target model
    auto_port: true
    local_host: "127.0.0.1"
    server_startup_timeout: 300  # Large models need more time
    
    # vLLM arguments for 8-GPU tensor parallelism
    vllm_args:
      tensor_parallel_size: 8                    # Use all 8 GPUs
      max_model_len: 2048                        # Reasonable context length
      gpu_memory_utilization: 0.9                # Use most of GPU memory
      dtype: "auto"                              # Let vLLM choose optimal precision
      trust_remote_code: false                   # Security
      seed: 42                                   # Reproducible results
      
      # Performance optimizations for multi-GPU
      pipeline_parallel_size: 1                 # Keep simple for now
      max_num_batched_tokens: 8192              # Allow larger batches
      max_num_seqs: 128                         # More concurrent sequences
      
    # Request batching configuration
    enable_batching: true                        # Enable request batching
    batch_size: 16                              # Process 16 requests per batch
    batch_timeout: 0.5                          # Wait max 0.5s to fill batch
  
  grader_shortname: "gpt41"
  model_config_file: "model_servers.yaml"

rag:
  enabled: false  # Disable for pure generation testing
  use_context_field: false

processing:
  parallel_workers: 8                           # 8 workers feeding the single server
  question_format: "auto"
  verbose: true
  random_selection: null                        # Process all questions
  random_seed: 42

output:
  save_incorrect: true
  output_directory: "."
  output_prefix: "multi_gpu_batch_results"

# Performance Notes:
# - Single vLLM server with tensor_parallel_size=8 distributes model across all GPUs
# - 8 parallel workers generate requests that get batched together
# - Request batching reduces I/O overhead and improves throughput
# - vLLM's internal batching handles multiple concurrent sequences efficiently
# - Expected throughput: High (limited mainly by model size and GPU memory) 