# Multi-GPU Batching Guide

This guide explains how to use the enhanced multi-GPU and request batching features for maximum throughput and compute utilization.

## üéØ What's Been Added

### ‚úÖ **Request Batching Support**
- **Automatic batching** of multiple requests to reduce I/O overhead
- **Configurable batch sizes** and timeouts
- **Background batch processor** thread for efficient request handling
- **Fallback to individual processing** if batching fails

### ‚úÖ **Multi-GPU Deployment Options**
- **Single server + tensor parallelism** (1 server across 8 GPUs)
- **Multiple servers** (4 servers √ó 2 GPUs each)
- **Hybrid approaches** for different workload patterns

### ‚úÖ **Enhanced Configuration**
- New `enable_batching`, `batch_size`, `batch_timeout` settings
- Integration with existing `parallel_workers` system
- **Smart fallback** when batching isn't available

## üöÄ Deployment Strategies

### Strategy 1: Single vLLM Server + Tensor Parallelism (Recommended)

**Best for:** Maximum throughput, large models, unified resource management

```yaml
# File: multi_gpu_batch_config.yaml
model:
  generator_settings:
    boot_local: true
    hf_model_id: "meta-llama/Llama-2-7b-chat-hf"
    
    # Multi-GPU tensor parallelism
    vllm_args:
      tensor_parallel_size: 8                    # Use all 8 GPUs
      max_model_len: 2048
      gpu_memory_utilization: 0.9
      max_num_batched_tokens: 8192              # Large internal batches
      max_num_seqs: 128                         # Many concurrent sequences
    
    # Request batching
    enable_batching: true
    batch_size: 16                              # 16 requests per batch
    batch_timeout: 0.5                          # Max 0.5s wait time

processing:
  parallel_workers: 8                           # 8 workers feeding batches
```

**Usage:**
```bash
python distllm/mcqa/rag_argonium_score_parallel_v2.py --config multi_gpu_batch_config.yaml
```

**How it works:**
1. **Single vLLM server** distributes model across 8 GPUs
2. **8 parallel workers** generate requests concurrently
3. **Batch processor** collects requests into batches of 16
4. **vLLM internal batching** handles multiple sequences efficiently

### Strategy 2: Multiple vLLM Servers

**Best for:** Fault isolation, different models, granular scaling

```bash
# Terminal 1 (GPUs 0,1)
export CUDA_VISIBLE_DEVICES=0,1
python script.py --config server1.yaml

# Terminal 2 (GPUs 2,3) 
export CUDA_VISIBLE_DEVICES=2,3
python script.py --config server2.yaml

# Terminal 3 (GPUs 4,5)
export CUDA_VISIBLE_DEVICES=4,5
python script.py --config server3.yaml

# Terminal 4 (GPUs 6,7)
export CUDA_VISIBLE_DEVICES=6,7
python script.py --config server4.yaml
```

## üìä Performance Comparison

| Strategy | Setup Complexity | Throughput | Memory Efficiency | Fault Tolerance |
|----------|------------------|------------|-------------------|-----------------|
| **Single Server + Tensor Parallel** | Low | **Highest** | **Best** | Lower |
| **Multiple Servers** | Medium | High | Good | **Best** |
| **Individual Requests** | Low | Lowest | Good | Medium |

## üîß Configuration Options

### Core Batching Settings

```yaml
generator_settings:
  enable_batching: true          # Enable request batching
  batch_size: 16                 # Requests per batch (tune this!)
  batch_timeout: 0.5             # Max wait time for batch to fill (seconds)
```

### vLLM Performance Settings

```yaml
vllm_args:
  # Multi-GPU configuration
  tensor_parallel_size: 8        # Number of GPUs to use
  
  # Memory and context
  max_model_len: 2048           # Context window
  gpu_memory_utilization: 0.9   # GPU memory to use
  
  # Batching optimization
  max_num_batched_tokens: 8192  # Total tokens per batch
  max_num_seqs: 128             # Concurrent sequences
  
  # Performance tuning
  dtype: "auto"                 # Precision (auto/half/float16)
  seed: 42                      # Reproducible results
```

## üéõÔ∏è Tuning Guidelines

### Batch Size Selection

| Model Size | Recommended batch_size | Notes |
|------------|----------------------|-------|
| **Small (< 1B params)** | 32-64 | Can handle large batches |
| **Medium (1-7B params)** | 16-32 | Balance memory vs throughput |
| **Large (7-13B params)** | 8-16 | Memory constrained |
| **Very Large (13B+ params)** | 4-8 | Conservative batching |

### Parallel Workers vs Batch Size

```yaml
# High throughput setup
processing:
  parallel_workers: 8
generator_settings:
  batch_size: 16
  batch_timeout: 0.3

# Memory-constrained setup  
processing:
  parallel_workers: 4
generator_settings:
  batch_size: 8
  batch_timeout: 0.5
```

## üìà Expected Performance Gains

### Throughput Improvements

- **Individual requests**: 1x baseline
- **Request batching**: **2-4x** improvement
- **Multi-GPU tensor parallel**: **6-8x** improvement  
- **Combined (batching + multi-GPU)**: **12-20x** improvement

### Real-world Example
```
# Single GPU, individual requests
Throughput: ~5 questions/second

# 8 GPUs with tensor parallelism + batching
Throughput: ~60-100 questions/second
```

## üîÑ How Batching Works

### Request Flow
1. **Workers submit requests** ‚Üí Batch queue
2. **Batch processor** collects requests until:
   - Batch is full (`batch_size` reached), OR
   - Timeout expires (`batch_timeout` seconds)
3. **Batch sent** to vLLM as concurrent requests
4. **Results distributed** back to waiting workers

### Monitoring Output
```
üöÄ Batch processing enabled (batch_size=16)
üöÄ Processing batch of 16 questions...
‚úÖ Generated 16 answers in 2.34s
   Throughput: 6.8 answers/sec
üìä Batch processing complete: 16/16 successful
```

## üõ†Ô∏è Troubleshooting

### Issue: Low Batch Utilization
**Symptoms:** Batches not filling up, low throughput
```
üöÄ Processing batch of 3 questions...  # Should be 16
```
**Solutions:**
1. **Increase `parallel_workers`** to generate more concurrent requests
2. **Reduce `batch_timeout`** to send partial batches faster  
3. **Increase total question count**

### Issue: Memory Errors
**Symptoms:** CUDA out of memory with batching
**Solutions:**
1. **Reduce `batch_size`**
2. **Lower `gpu_memory_utilization`**
3. **Decrease `max_num_batched_tokens`**

### Issue: Batching Disabled
**Symptoms:** Falls back to individual processing
**Check:**
1. `enable_batching: true` in config
2. vLLM server supports batching
3. No configuration errors

## üéØ Best Practices

### 1. Start Simple
```yaml
# Begin with conservative settings
enable_batching: true
batch_size: 8
batch_timeout: 1.0
parallel_workers: 4
```

### 2. Monitor GPU Utilization
```bash
# Watch GPU usage in real-time
watch -n 1 nvidia-smi
```

### 3. Tune Iteratively
1. **Establish baseline** with small batches
2. **Increase batch_size** until memory limit
3. **Adjust parallel_workers** for optimal batching
4. **Fine-tune timeouts** for your workload

### 4. Production Deployment
```yaml
# Production-ready configuration
generator_settings:
  enable_batching: true
  batch_size: 16
  batch_timeout: 0.3
  server_startup_timeout: 300

vllm_args:
  tensor_parallel_size: 8
  gpu_memory_utilization: 0.85  # Leave some headroom
  max_num_seqs: 64
  
processing:
  parallel_workers: 8
```

## üìã Quick Start Checklist

- [ ] **Install vLLM**: `pip install vllm`
- [ ] **Choose strategy**: Single server vs multiple servers
- [ ] **Set GPU count**: `tensor_parallel_size`
- [ ] **Enable batching**: `enable_batching: true`
- [ ] **Tune batch size**: Start with 8-16
- [ ] **Set parallel workers**: Match or exceed batch size
- [ ] **Monitor performance**: Watch nvidia-smi + logs
- [ ] **Iterate**: Adjust based on memory and throughput

## üîó Example Commands

```bash
# Test with small model first
python distllm/mcqa/rag_argonium_score_parallel_v2.py \
  --config sample_local_vllm_config_enhanced.yaml

# Production run with large model + batching
python distllm/mcqa/rag_argonium_score_parallel_v2.py \
  --config multi_gpu_batch_config.yaml

# Monitor GPU usage while running
watch -n 1 'nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv'
```

The batching + multi-GPU setup should give you **dramatic throughput improvements** for your 8-GPU system! üöÄ 