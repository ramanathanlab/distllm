# An input directory containing the files to embed.
input_dir: /lus/flare/projects/FoundEpidem/ogokdemir/projects/data/part1/embeddings
# An output directory to save the embeddings.
output_dir: /lus/flare/projects/FoundEpidem/ogokdemir/projects/data/part1/embeddings/question_samples
# A set of glob patterns to match the input files.
glob_patterns: ['*']

# Settings for the prompt.
prompt_config:
  name: question_chunk

# Settings for the reader.
reader_config:
  name: huggingface

# Settings for the writer.
writer_config:
  name: huggingface

# Settings for the generator.
generator_config:
  name: vllm
  llm_name: mistralai/Mistral-7B-Instruct-v0.2
  top_p: 0.95

# The compute settings for the workflow
compute_config:
  # The name of the compute platform to use
  name: aurora
  # The number of compute nodes to use
  num_nodes: 4
  # Make sure to update the path to your conda environment and HF cache
  worker_init: |
    module load frameworks
    source /home/ogokdemir/distllm-venv/bin/activate
    export PYTHONPATH=/opt/aurora/24.347.0/frameworks/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/site-packages:$PYTHONPATH
    export HF_HOME=/lus/flare/projects/FoundEpidem/ogokdemir/hf_home
  # The scheduler options to use when submitting jobs
  scheduler_options: "#PBS -l filesystems=home:flare"
  # Make sure to change the account to the account you want to charge
  account: FoundEpidem
  # The HPC queue to submit to
  queue: debug-scaling
  # The amount of time to request for your job
  walltime: "01:00:00"