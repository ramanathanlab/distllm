Loading configuration from ../../examples/mcqa/reasoning_traces/rag_chunks/pubmedbert/qwen15-14b-chat-rag-chunks.yaml
Configuration saved for experiment tracking: /homes/ogokdemir/lambda_stor/ArgoniumRick/reasoning-evals/rag-chunks/pubmedbert/qwen15-14b-chat/qwen15-14b-chat_rag_chunks_checkpointed_config_Qwen_Qwen1.5-14B-Chat_20250812_133008.yaml
Using inline retriever configuration
Found available port: 8000
🔍 Validating model 'Qwen/Qwen1.5-14B-Chat' availability...
⚠️  Warning: Could not verify model 'Qwen/Qwen1.5-14B-Chat' on HuggingFace (status: 307)
   This might be a private model or network issue - continuing anyway...
🚀 Starting local vLLM server with command:
   /nfs/lambda_stor_01/homes/ac.ogokdemir/miniforge3/envs/distllm-env/bin/python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen1.5-14B-Chat --host 127.0.0.1 --port 8000 --tensor_parallel_size 8 --max_model_len 32768 --gpu_memory_utilization 0.35 --dtype auto --seed 42 --pipeline_parallel_size 1 --max_num_batched_tokens 32768 --max_num_seqs 512
📋 Model: Qwen/Qwen1.5-14B-Chat
🌐 Address: 127.0.0.1:8000
📝 Logs will be written to:
   STDOUT: vllm_logs/vllm_stdout_20250812_133009.log
   STDERR: vllm_logs/vllm_stderr_20250812_133009.log
⏳ Waiting for vLLM server to start (timeout: 600s)...
📊 Progress indicators:
   - Model download/loading
   - GPU memory allocation
   - Server initialization
   - API endpoint readiness
🔄 Starting health checks every 2.0s...
📤 vLLM: INFO 08-12 13:30:18 [cli_args.py:261] non-default args: {'host': '127.0.0.1', 'model': 'Qwen/Qwen1.5-14B-Chat', 'seed': 42, 'max_model_len': 32768, 'tensor_parallel_size': 8, 'gpu_memory_utilization': 0.35, 'max_num_batched_tokens': 32768, 'max_num_seqs': 512}
⏱️  Still waiting... (10s / 600s)
📊 System monitoring: psutil/GPUtil not available
⏱️  Still waiting... (20s / 600s)
📊 System monitoring: psutil/GPUtil not available
📤 vLLM: INFO 08-12 13:30:30 [config.py:1604] Using max model len 32768
⏱️  Still waiting... (30s / 600s)
📊 System monitoring: psutil/GPUtil not available
📤 vLLM: INFO 08-12 13:30:42 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='Qwen/Qwen1.5-14B-Chat', speculative_config=None, tokenizer='Qwen/Qwen1.5-14B-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=Qwen/Qwen1.5-14B-Chat, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
📤 vLLM: WARNING 08-12 13:30:42 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 192 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
⏱️  Still waiting... (40s / 600s)
📊 System monitoring: psutil/GPUtil not available
⏱️  Still waiting... (50s / 600s)
📊 System monitoring: psutil/GPUtil not available
⏱️  Still waiting... (60s / 600s)
📊 System monitoring: psutil/GPUtil not available
⏱️  Still waiting... (70s / 600s)
📊 System monitoring: psutil/GPUtil not available
⏱️  Still waiting... (80s / 600s)
📊 System monitoring: psutil/GPUtil not available
📤 vLLM: [1;36m(VllmWorker rank=5 pid=3801744)[0;0m WARNING 08-12 13:31:38 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
📤 vLLM: [1;36m(VllmWorker rank=4 pid=3801743)[0;0m WARNING 08-12 13:31:38 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
📤 vLLM: [1;36m(VllmWorker rank=6 pid=3801745)[0;0m WARNING 08-12 13:31:38 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
📤 vLLM: [1;36m(VllmWorker rank=3 pid=3801742)[0;0m WARNING 08-12 13:31:38 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
📤 vLLM: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m WARNING 08-12 13:31:38 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
📤 vLLM: [1;36m(VllmWorker rank=2 pid=3801741)[0;0m WARNING 08-12 13:31:38 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
📤 vLLM: [1;36m(VllmWorker rank=7 pid=3801746)[0;0m WARNING 08-12 13:31:38 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
📤 vLLM: [1;36m(VllmWorker rank=1 pid=3801740)[0;0m WARNING 08-12 13:31:38 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
📤 vLLM: [1;36m(VllmWorker rank=6 pid=3801745)[0;0m WARNING 08-12 13:31:39 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
📤 vLLM: [1;36m(VllmWorker rank=2 pid=3801741)[0;0m WARNING 08-12 13:31:39 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
📤 vLLM: [1;36m(VllmWorker rank=3 pid=3801742)[0;0m WARNING 08-12 13:31:39 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
📤 vLLM: [1;36m(VllmWorker rank=4 pid=3801743)[0;0m WARNING 08-12 13:31:39 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
📤 vLLM: [1;36m(VllmWorker rank=5 pid=3801744)[0;0m WARNING 08-12 13:31:39 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
📤 vLLM: [1;36m(VllmWorker rank=1 pid=3801740)[0;0m WARNING 08-12 13:31:39 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
📤 vLLM: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m WARNING 08-12 13:31:39 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
📤 vLLM: [1;36m(VllmWorker rank=7 pid=3801746)[0;0m WARNING 08-12 13:31:39 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
📤 vLLM: [1;36m(VllmWorker rank=3 pid=3801742)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen1.5-14B-Chat...
📤 vLLM: [1;36m(VllmWorker rank=4 pid=3801743)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen1.5-14B-Chat...
📤 vLLM: [1;36m(VllmWorker rank=5 pid=3801744)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen1.5-14B-Chat...
📤 vLLM: [1;36m(VllmWorker rank=2 pid=3801741)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen1.5-14B-Chat...
📤 vLLM: [1;36m(VllmWorker rank=1 pid=3801740)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen1.5-14B-Chat...
📤 vLLM: [1;36m(VllmWorker rank=6 pid=3801745)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen1.5-14B-Chat...
📤 vLLM: [1;36m(VllmWorker rank=7 pid=3801746)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen1.5-14B-Chat...
📤 vLLM: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen1.5-14B-Chat...
📤 vLLM: [1;36m(VllmWorker rank=5 pid=3801744)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1875] Loading model from scratch...
📤 vLLM: [1;36m(VllmWorker rank=6 pid=3801745)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1875] Loading model from scratch...
📤 vLLM: [1;36m(VllmWorker rank=4 pid=3801743)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1875] Loading model from scratch...
📤 vLLM: [1;36m(VllmWorker rank=7 pid=3801746)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1875] Loading model from scratch...
📤 vLLM: [1;36m(VllmWorker rank=3 pid=3801742)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1875] Loading model from scratch...
📤 vLLM: [1;36m(VllmWorker rank=1 pid=3801740)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1875] Loading model from scratch...
📤 vLLM: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1875] Loading model from scratch...
📤 vLLM: [1;36m(VllmWorker rank=2 pid=3801741)[0;0m INFO 08-12 13:31:39 [gpu_model_runner.py:1875] Loading model from scratch...
⏱️  Still waiting... (90s / 600s)
📊 System monitoring: psutil/GPUtil not available
📤 vLLM: [1;36m(VllmWorker rank=6 pid=3801745)[0;0m INFO 08-12 13:31:39 [weight_utils.py:296] Using model weights format ['*.safetensors']
📤 vLLM: [1;36m(VllmWorker rank=5 pid=3801744)[0;0m INFO 08-12 13:31:39 [weight_utils.py:296] Using model weights format ['*.safetensors']
📤 vLLM: [1;36m(VllmWorker rank=4 pid=3801743)[0;0m INFO 08-12 13:31:39 [weight_utils.py:296] Using model weights format ['*.safetensors']
📤 vLLM: [1;36m(VllmWorker rank=3 pid=3801742)[0;0m INFO 08-12 13:31:39 [weight_utils.py:296] Using model weights format ['*.safetensors']
📤 vLLM: [1;36m(VllmWorker rank=7 pid=3801746)[0;0m INFO 08-12 13:31:39 [weight_utils.py:296] Using model weights format ['*.safetensors']
📤 vLLM: [1;36m(VllmWorker rank=1 pid=3801740)[0;0m INFO 08-12 13:31:39 [weight_utils.py:296] Using model weights format ['*.safetensors']
📤 vLLM: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m INFO 08-12 13:31:39 [weight_utils.py:296] Using model weights format ['*.safetensors']
📤 vLLM: [1;36m(VllmWorker rank=2 pid=3801741)[0;0m INFO 08-12 13:31:39 [weight_utils.py:296] Using model weights format ['*.safetensors']
🚨 vLLM Error: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m
🚨 vLLM Error: Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
🚨 vLLM Error: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m
🚨 vLLM Error: Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:00<00:00,  9.40it/s]
🚨 vLLM Error: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m
🚨 vLLM Error: Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:00<00:00,  6.54it/s]
🚨 vLLM Error: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m
🚨 vLLM Error: Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:00<00:00,  5.15it/s]
🚨 vLLM Error: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m
🚨 vLLM Error: Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:00<00:00,  5.06it/s]
📤 vLLM: [1;36m(VllmWorker rank=7 pid=3801746)[0;0m INFO 08-12 13:31:41 [weight_utils.py:312] Time spent downloading weights for Qwen/Qwen1.5-14B-Chat: 0.580743 seconds
🚨 vLLM Error: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m
🚨 vLLM Error: Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:01<00:00,  4.72it/s]
🚨 vLLM Error: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m
🚨 vLLM Error: Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:01<00:00,  4.28it/s]
📤 vLLM: [1;36m(VllmWorker rank=6 pid=3801745)[0;0m INFO 08-12 13:31:41 [default_loader.py:262] Loading weights took 1.76 seconds
🚨 vLLM Error: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m
🚨 vLLM Error: Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:01<00:00,  4.09it/s]
🚨 vLLM Error: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m
🚨 vLLM Error: Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:01<00:00,  4.74it/s]
🚨 vLLM Error: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m
📤 vLLM: [1;36m(VllmWorker rank=1 pid=3801740)[0;0m INFO 08-12 13:31:41 [default_loader.py:262] Loading weights took 1.88 seconds
📤 vLLM: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m INFO 08-12 13:31:41 [default_loader.py:262] Loading weights took 1.77 seconds
📤 vLLM: [1;36m(VllmWorker rank=6 pid=3801745)[0;0m INFO 08-12 13:31:42 [gpu_model_runner.py:1892] Model loading took 3.3299 GiB and 2.290059 seconds
📤 vLLM: [1;36m(VllmWorker rank=2 pid=3801741)[0;0m INFO 08-12 13:31:42 [default_loader.py:262] Loading weights took 1.77 seconds
📤 vLLM: [1;36m(VllmWorker rank=5 pid=3801744)[0;0m INFO 08-12 13:31:42 [default_loader.py:262] Loading weights took 1.80 seconds
📤 vLLM: [1;36m(VllmWorker rank=1 pid=3801740)[0;0m INFO 08-12 13:31:42 [gpu_model_runner.py:1892] Model loading took 3.3299 GiB and 2.532198 seconds
📤 vLLM: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m INFO 08-12 13:31:42 [gpu_model_runner.py:1892] Model loading took 3.3299 GiB and 2.596740 seconds
📤 vLLM: [1;36m(VllmWorker rank=2 pid=3801741)[0;0m INFO 08-12 13:31:42 [gpu_model_runner.py:1892] Model loading took 3.3299 GiB and 2.736201 seconds
📤 vLLM: [1;36m(VllmWorker rank=5 pid=3801744)[0;0m INFO 08-12 13:31:42 [gpu_model_runner.py:1892] Model loading took 3.3299 GiB and 3.048625 seconds
📤 vLLM: [1;36m(VllmWorker rank=7 pid=3801746)[0;0m INFO 08-12 13:31:42 [default_loader.py:262] Loading weights took 1.72 seconds
📤 vLLM: [1;36m(VllmWorker rank=3 pid=3801742)[0;0m INFO 08-12 13:31:43 [default_loader.py:262] Loading weights took 1.75 seconds
📤 vLLM: [1;36m(VllmWorker rank=4 pid=3801743)[0;0m INFO 08-12 13:31:43 [default_loader.py:262] Loading weights took 1.78 seconds
📤 vLLM: [1;36m(VllmWorker rank=7 pid=3801746)[0;0m INFO 08-12 13:31:43 [gpu_model_runner.py:1892] Model loading took 3.3299 GiB and 3.606912 seconds
📤 vLLM: [1;36m(VllmWorker rank=3 pid=3801742)[0;0m INFO 08-12 13:31:43 [gpu_model_runner.py:1892] Model loading took 3.3299 GiB and 3.830116 seconds
📤 vLLM: [1;36m(VllmWorker rank=4 pid=3801743)[0;0m INFO 08-12 13:31:43 [gpu_model_runner.py:1892] Model loading took 3.3299 GiB and 4.016167 seconds
⏱️  Still waiting... (100s / 600s)
📊 System monitoring: psutil/GPUtil not available
⏱️  Still waiting... (111s / 600s)
📊 System monitoring: psutil/GPUtil not available
⏱️  Still waiting... (121s / 600s)
📊 System monitoring: psutil/GPUtil not available
⏱️  Still waiting... (131s / 600s)
📊 System monitoring: psutil/GPUtil not available
📤 vLLM: [1;36m(VllmWorker rank=6 pid=3801745)[0;0m INFO 08-12 13:32:28 [gpu_worker.py:255] Available KV cache memory: 25.69 GiB
📤 vLLM: [1;36m(VllmWorker rank=7 pid=3801746)[0;0m INFO 08-12 13:32:28 [gpu_worker.py:255] Available KV cache memory: 25.69 GiB
📤 vLLM: [1;36m(VllmWorker rank=4 pid=3801743)[0;0m INFO 08-12 13:32:28 [gpu_worker.py:255] Available KV cache memory: 25.69 GiB
📤 vLLM: [1;36m(VllmWorker rank=5 pid=3801744)[0;0m INFO 08-12 13:32:28 [gpu_worker.py:255] Available KV cache memory: 25.69 GiB
📤 vLLM: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m INFO 08-12 13:32:28 [gpu_worker.py:255] Available KV cache memory: 25.69 GiB
📤 vLLM: [1;36m(VllmWorker rank=2 pid=3801741)[0;0m INFO 08-12 13:32:28 [gpu_worker.py:255] Available KV cache memory: 25.69 GiB
📤 vLLM: [1;36m(VllmWorker rank=1 pid=3801740)[0;0m INFO 08-12 13:32:28 [gpu_worker.py:255] Available KV cache memory: 25.69 GiB
📤 vLLM: [1;36m(VllmWorker rank=3 pid=3801742)[0;0m INFO 08-12 13:32:28 [gpu_worker.py:255] Available KV cache memory: 25.69 GiB
📤 vLLM: INFO 08-12 13:32:29 [kv_cache_utils.py:833] GPU KV cache size: 269,376 tokens
📤 vLLM: INFO 08-12 13:32:29 [kv_cache_utils.py:833] GPU KV cache size: 269,376 tokens
📤 vLLM: INFO 08-12 13:32:29 [kv_cache_utils.py:833] GPU KV cache size: 269,376 tokens
📤 vLLM: INFO 08-12 13:32:29 [kv_cache_utils.py:833] GPU KV cache size: 269,376 tokens
📤 vLLM: INFO 08-12 13:32:29 [kv_cache_utils.py:833] GPU KV cache size: 269,376 tokens
📤 vLLM: INFO 08-12 13:32:29 [kv_cache_utils.py:833] GPU KV cache size: 269,376 tokens
📤 vLLM: INFO 08-12 13:32:29 [kv_cache_utils.py:833] GPU KV cache size: 269,376 tokens
📤 vLLM: INFO 08-12 13:32:29 [kv_cache_utils.py:833] GPU KV cache size: 269,376 tokens
🚨 vLLM Error: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m
⏱️  Still waiting... (141s / 600s)
📊 System monitoring: psutil/GPUtil not available
🚨 vLLM Error: Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:   3%|▎         | 2/67 [00:00<00:05, 11.77it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:   6%|▌         | 4/67 [00:00<00:04, 13.43it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:   9%|▉         | 6/67 [00:00<00:04, 13.78it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  12%|█▏        | 8/67 [00:00<00:08,  7.36it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  15%|█▍        | 10/67 [00:01<00:06,  8.93it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  18%|█▊        | 12/67 [00:01<00:05, 10.36it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  21%|██        | 14/67 [00:01<00:04, 11.50it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  24%|██▍       | 16/67 [00:01<00:04, 12.51it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  27%|██▋       | 18/67 [00:01<00:03, 13.08it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  30%|██▉       | 20/67 [00:01<00:03, 13.76it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  33%|███▎      | 22/67 [00:01<00:03, 14.27it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  36%|███▌      | 24/67 [00:01<00:02, 14.43it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  39%|███▉      | 26/67 [00:02<00:02, 14.40it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  42%|████▏     | 28/67 [00:02<00:02, 14.80it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  45%|████▍     | 30/67 [00:02<00:02, 15.02it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  48%|████▊     | 32/67 [00:02<00:02, 15.16it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  51%|█████     | 34/67 [00:02<00:02, 14.95it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  54%|█████▎    | 36/67 [00:02<00:02, 15.19it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  57%|█████▋    | 38/67 [00:02<00:01, 15.39it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  60%|█████▉    | 40/67 [00:03<00:01, 15.76it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  63%|██████▎   | 42/67 [00:03<00:01, 15.77it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  66%|██████▌   | 44/67 [00:03<00:01, 15.94it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  69%|██████▊   | 46/67 [00:03<00:01, 15.91it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  72%|███████▏  | 48/67 [00:03<00:01, 16.05it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  75%|███████▍  | 50/67 [00:03<00:01, 16.08it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  78%|███████▊  | 52/67 [00:03<00:00, 16.02it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  81%|████████  | 54/67 [00:03<00:00, 15.96it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  84%|████████▎ | 56/67 [00:04<00:00, 16.01it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  87%|████████▋ | 58/67 [00:04<00:00, 16.20it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  90%|████████▉ | 60/67 [00:04<00:00, 16.45it/s]
🚨 vLLM Error: Capturing CUDA graph shapes:  93%|█████████▎| 62/67 [00:04<00:00, 17.02it/s]
📤 vLLM: [1;36m(VllmWorker rank=6 pid=3801745)[0;0m INFO 08-12 13:32:34 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 1.00 GiB
🚨 vLLM Error: Capturing CUDA graph shapes:  96%|█████████▌| 64/67 [00:04<00:00, 17.71it/s]
🚨 vLLM Error: Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:04<00:00, 19.08it/s]
🚨 vLLM Error: Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:04<00:00, 14.58it/s]
📤 vLLM: [1;36m(VllmWorker rank=4 pid=3801743)[0;0m INFO 08-12 13:32:34 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 1.00 GiB
📤 vLLM: [1;36m(VllmWorker rank=0 pid=3801739)[0;0m INFO 08-12 13:32:34 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 1.00 GiB
📤 vLLM: [1;36m(VllmWorker rank=3 pid=3801742)[0;0m INFO 08-12 13:32:34 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 1.00 GiB
📤 vLLM: [1;36m(VllmWorker rank=7 pid=3801746)[0;0m INFO 08-12 13:32:34 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 1.00 GiB
📤 vLLM: [1;36m(VllmWorker rank=2 pid=3801741)[0;0m INFO 08-12 13:32:34 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 1.00 GiB
📤 vLLM: [1;36m(VllmWorker rank=5 pid=3801744)[0;0m INFO 08-12 13:32:34 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 1.00 GiB
📤 vLLM: [1;36m(VllmWorker rank=1 pid=3801740)[0;0m INFO 08-12 13:32:34 [gpu_model_runner.py:2485] Graph capturing finished in 5 secs, took 1.00 GiB
📤 vLLM: INFO 08-12 13:32:34 [core.py:193] init engine (profile, create kv cache, warmup model) took 50.62 seconds
📤 vLLM: INFO 08-12 13:32:35 [loggers.py:141] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 16836
📤 vLLM: WARNING 08-12 13:32:35 [config.py:1528] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
📤 vLLM: INFO 08-12 13:32:35 [serving_responses.py:89] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
📤 vLLM: INFO 08-12 13:32:35 [serving_chat.py:122] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
📤 vLLM: INFO 08-12 13:32:35 [serving_completion.py:77] Using default completion sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
📤 vLLM: INFO 08-12 13:32:35 [launcher.py:37] Route: /v1/models, Methods: GET
🚨 vLLM Error: INFO:     Started server process [3801260]
🚨 vLLM Error: INFO:     Waiting for application startup.
🚨 vLLM Error: INFO:     Application startup complete.
🔗 Port 8000 is accepting connections
